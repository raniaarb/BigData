{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T14:31:54.762393Z",
     "iopub.status.busy": "2025-10-25T14:31:54.762156Z",
     "iopub.status.idle": "2025-10-25T14:31:54.771634Z",
     "shell.execute_reply": "2025-10-25T14:31:54.770534Z",
     "shell.execute_reply.started": "2025-10-25T14:31:54.762372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meta-kaggle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# استعراض جميع المجلدات داخل /kaggle/input\n",
    "print(os.listdir(\"/kaggle/input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T14:32:41.740175Z",
     "iopub.status.busy": "2025-10-25T14:32:41.739890Z",
     "iopub.status.idle": "2025-10-25T14:32:41.756206Z",
     "shell.execute_reply": "2025-10-25T14:32:41.755400Z",
     "shell.execute_reply.started": "2025-10-25T14:32:41.740155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KernelTags.csv', 'ModelVariations.csv', 'KernelVersionCompetitionSources.csv', 'Datasets.csv', 'KernelVersionKernelSources.csv', 'KernelVotes.csv', 'Submissions.csv', 'KernelLanguages.csv', 'Users.csv', 'ForumMessageVotes.csv', 'Competitions.csv', 'DatasetTaskSubmissions.csv', 'UserAchievements.csv', 'UserOrganizations.csv', 'Teams.csv', 'UserFollowers.csv', 'CompetitionTags.csv', 'Kernels.csv', 'Organizations.csv', 'Datasources.csv', 'ModelVersions.csv', 'ForumTopics.csv', 'DatasetVersions.csv', 'ModelVotes.csv', 'DatasetVotes.csv', 'TeamMemberships.csv', 'Forums.csv', 'KernelVersions.csv', 'ModelVariationVersions.csv', 'ForumMessages.csv', 'KernelVersionDatasetSources.csv', 'Episodes.csv', 'EpisodeAgents.csv', 'KernelAcceleratorTypes.csv', 'KernelVersionModelSources.csv', 'ForumMessageReactions.csv', 'Tags.csv', 'DatasetTasks.csv', 'Models.csv', 'DatasetTags.csv', 'ModelTags.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/kaggle/input/meta-kaggle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T15:33:05.947002Z",
     "iopub.status.busy": "2025-10-25T15:33:05.945107Z",
     "iopub.status.idle": "2025-10-25T15:35:27.892860Z",
     "shell.execute_reply": "2025-10-25T15:35:27.891710Z",
     "shell.execute_reply.started": "2025-10-25T15:33:05.946962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas read done in 141.93s\n",
      "Memory used: 21764.79 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time, psutil\n",
    "\n",
    "def memory_usage_mb():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "files_to_read = [\n",
    "    \"Kernels.csv\",\n",
    "    \"KernelVersions.csv\",\n",
    "    \"Datasets.csv\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for f in files_to_read:\n",
    "    file_path = f\"/kaggle/input/meta-kaggle/{f}\"\n",
    "    chunks = pd.read_csv(file_path, chunksize=100000)\n",
    "    df = pd.concat(chunks)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_pandas = pd.concat(dfs, axis=0)\n",
    "print(f\"Pandas read done in {time.time() - start:.2f}s\")\n",
    "print(f\"Memory used: {memory_usage_mb():.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T15:38:29.560260Z",
     "iopub.status.busy": "2025-10-25T15:38:29.559482Z",
     "iopub.status.idle": "2025-10-25T15:38:31.395826Z",
     "shell.execute_reply": "2025-10-25T15:38:31.394896Z",
     "shell.execute_reply.started": "2025-10-25T15:38:29.560230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask preview:\n",
      "        Id  AuthorUserId  CurrentKernelVersionId  ForkParentKernelVersionId  \\\n",
      "0    297.0         368.0                  1760.0                     1717.0   \n",
      "1    314.0         368.0                  1810.0                        NaN   \n",
      "2    316.0         368.0                  1809.0                        NaN   \n",
      "3    599.0         368.0                  2802.0                     1275.0   \n",
      "4  15297.0         368.0                 55821.0                        NaN   \n",
      "\n",
      "   ForumTopicId  FirstKernelVersionId         CreationDate EvaluationDate  \\\n",
      "0           NaN                1750.0  04/16/2015 17:42:31     04/16/2015   \n",
      "1           NaN                1783.0  04/17/2015 23:33:44     04/18/2015   \n",
      "2       17103.0                1806.0  04/18/2015 00:14:28     04/18/2015   \n",
      "3       51451.0                2523.0  04/24/2015 16:55:05     04/25/2015   \n",
      "4           NaN               55817.0  08/26/2015 01:53:50     08/26/2015   \n",
      "\n",
      "  MadePublicDate  IsProjectLanguageTemplate  \\\n",
      "0     04/16/2015                      False   \n",
      "1     04/17/2015                      False   \n",
      "2     04/18/2015                      False   \n",
      "3     04/24/2015                      False   \n",
      "4     08/26/2015                      False   \n",
      "\n",
      "                            CurrentUrlSlug  Medal MedalAwardDate  TotalViews  \\\n",
      "0  rf-proximity-with-improved-color-palate    NaN           <NA>      1231.0   \n",
      "1         most-common-passenger-firstnames    NaN           <NA>      1665.0   \n",
      "2                              lucky-names    3.0     07/07/2025     13024.0   \n",
      "3                  random-forest-benchmark    NaN           <NA>      4455.0   \n",
      "4                                testdebug    NaN           <NA>       257.0   \n",
      "\n",
      "   TotalComments  TotalVotes  \n",
      "0            0.0         8.0  \n",
      "1            0.0         3.0  \n",
      "2           10.0        72.0  \n",
      "3            1.0        20.0  \n",
      "4            0.0         1.0  \n",
      "Dask read time: 1.83s\n",
      "Memory used: 21795.37 MB\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "start = time.time()\n",
    "paths = [f\"/kaggle/input/meta-kaggle/{f}\" for f in files_to_read]\n",
    "df_dask = dd.read_csv(paths, assume_missing=True)\n",
    "print(\"Dask preview:\")\n",
    "print(df_dask.head())\n",
    "\n",
    "print(f\"Dask read time: {time.time() - start:.2f}s\")\n",
    "print(f\"Memory used: {memory_usage_mb():.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T14:51:23.451629Z",
     "iopub.status.busy": "2025-10-25T14:51:23.451182Z",
     "iopub.status.idle": "2025-10-25T15:05:15.016016Z",
     "shell.execute_reply": "2025-10-25T15:05:15.014973Z",
     "shell.execute_reply.started": "2025-10-25T14:51:23.451598Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compressing Kernels.csv ...\n",
      " Done: /kaggle/working/Kernels.csv.gz  |  Time: 90.84s\n",
      "\n",
      " Compressing KernelVersions.csv ...\n",
      " Done: /kaggle/working/KernelVersions.csv.gz  |  Time: 706.76s\n",
      "\n",
      " Compressing Datasets.csv ...\n",
      " Done: /kaggle/working/Datasets.csv.gz  |  Time: 33.95s\n",
      "\n",
      "All selected files compressed successfully!\n",
      "\n",
      "Compressed files: ['Datasets.csv.gz', 'KernelVersions.csv.gz', 'Kernels.csv.gz', '.virtual_documents']\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, shutil, time, psutil, pandas as pd\n",
    "\n",
    "folder = \"/kaggle/input/meta-kaggle\"\n",
    "\n",
    "\n",
    "files_to_compress = [\n",
    "    \"Kernels.csv\",         \n",
    "    \"KernelVersions.csv\",   \n",
    "    \"Datasets.csv\",         \n",
    "]\n",
    "\n",
    "# دالة حساب استهلاك الذاكرة\n",
    "def memory_usage_mb():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "# ضغط الملفات وحفظها في working directory\n",
    "for f in files_to_compress:\n",
    "    src = f\"{folder}/{f}\"\n",
    "    dst = f\"/kaggle/working/{f}.gz\"\n",
    "\n",
    "    print(f\" Compressing {f} ...\")\n",
    "    start = time.time()\n",
    "    with open(src, \"rb\") as fin, gzip.open(dst, \"wb\") as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "    print(f\" Done: {dst}  |  Time: {time.time() - start:.2f}s\\n\")\n",
    "\n",
    "print(\"All selected files compressed successfully!\\n\")\n",
    "print(\"Compressed files:\", os.listdir(\"/kaggle/working\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T15:16:15.986030Z",
     "iopub.status.busy": "2025-10-25T15:16:15.984646Z",
     "iopub.status.idle": "2025-10-25T15:18:30.725184Z",
     "shell.execute_reply": "2025-10-25T15:18:30.723949Z",
     "shell.execute_reply.started": "2025-10-25T15:16:15.985993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Reading compressed file: /kaggle/working/Kernels.csv.gz\n",
      "📘 Reading compressed file: /kaggle/working/KernelVersions.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/1649400246.py:8: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(gz_path, compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Reading compressed file: /kaggle/working/Datasets.csv.gz\n",
      "\n",
      " Compression Method Finished Successfully!\n",
      "Time taken: 134.73 seconds\n",
      " Memory used: 8860.53 MB\n",
      " Total rows read: 18,265,835\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_mem = memory_usage_mb()\n",
    "\n",
    "dfs = []\n",
    "for f in files_to_compress:\n",
    "    gz_path = f\"/kaggle/working/{f}.gz\"\n",
    "    print(f\" Reading compressed file: {gz_path}\")\n",
    "    df = pd.read_csv(gz_path, compression='gzip')\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, axis=0)\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "used_mem = memory_usage_mb() - start_mem\n",
    "\n",
    "print(\"\\n Compression Method Finished Successfully!\")\n",
    "print(f\"Time taken: {read_time:.2f} seconds\")\n",
    "print(f\" Memory used: {used_mem:.2f} MB\")\n",
    "print(f\" Total rows read: {len(df_all):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "تحليل النتائج\n",
    "1. من حيث السرعة\n",
    "\n",
    "Dask :هي الأسرع في مرحلة التحميل (لأنها لا تقرأ كل البيانات دفعة واحدة)\n",
    "\n",
    "2. من حيث استهلاك الذاكرة\n",
    "\n",
    "Pandas :حمّلت كل الملفات في الذاكرة  GB 21.7 \n",
    "Dask : حافظ على نفس المستوى تقريبًا لأنه في النهاية يجب أن يحمل البيانات للمعالجة.\n",
    "\n",
    "طريقة الضغط (compression) خفّضت الذاكرة المستعملة إلى أقل من النصف (≈8.8 GB) .\n",
    "خلاصة القول في حالة الملفات الضخمة  نجد اسرع طرقة للقراءة هي dask read method و اقلهم من ناحية الذاكرة هي compression read method"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9,
     "sourceId": 13493145,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
